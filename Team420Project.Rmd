---
title: 'Team 420 Final Project'
author: "Ruben Verghese, Simer Singh, Gautam Ajjarapu, James Wei"
date: 'December 14, 2020'
output:
  html_document:
    theme: readable
    toc: yes
---
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

# Introduction
```{r, warning=FALSE}
library(lmtest)
library(faraway)
library(corrplot)
```

## Data Cleaning
```{r, warning=FALSE}
data = read.csv("county_facts.csv", stringsAsFactors = FALSE)
results = read.csv("primary_results.csv", stringsAsFactors = FALSE)
library(stringr)
predictors = data[data$state_abbreviation != "", ]
for (i in 1:nrow(predictors)) { 
  predictors$area_name[i] = str_replace(predictors$area_name[i], " County", "")
  predictors$area_name[i] = str_replace(predictors$area_name[i], " Parish", "")
  #print(predictors$area_name[i])
}

for (i in 1:nrow(predictors)) {
  state = predictors$state_abbreviation[i]
  fips = predictors$fips[i]
  frac_vote = results[results$state_abbreviation == state & results$fips == fips & results$candidate == "Hillary Clinton",]$fraction_votes
  #print(frac_vote)
  if (length(frac_vote) == 0 || is.na(frac_vote)) {
    #print(state)
    #print(fips)
  } else {
    predictors$Hillary[i] = frac_vote
  }
  county = predictors$area_name[i]
  frac_vote = results[results$county == county & results$candidate == "Hillary Clinton",]$fraction_votes
  if (predictors$Hillary[i] == 0) {
    if (length(frac_vote) == 0 || is.na(frac_vote)) {
    #print(county)
    } else {
    predictors$Hillary[i] = frac_vote
    }
  }
}

final_table = predictors[predictors$Hillary != 0 | predictors$state_abbreviation != "DC", ]

final_table$Hillary <- final_table$Hillary * 100 # convert to percent

south = c('TX', 'OK', 'LA', 'AR', 'MS', 'AL', 'TN', 'KY', 'WV', 'VA', 'NC', 'SC', 'GA', 'AL', 'FL', 'MD', 'DC', 'DE')
west = c('CA', 'OR', 'WA', 'ID', 'MT', 'NV', 'AZ', 'UT', 'CO', 'NM', 'WY', 'AK')
midwest = c('ND', 'SD', 'NE', 'KS', 'MN', 'IA', 'MO', 'WI', 'IL', 'IN', 'MI', 'OH')
northeast = c('PA', 'NJ', 'CT', 'RI', 'MA', 'VT', 'NY', 'NH', 'ME')

for (c in 1:nrow(final_table)) {
  if (final_table[c,]$state_abbreviation %in% south) {
      final_table[c,]$state_abbreviation = 'S'
  } else if (final_table[c,]$state_abbreviation %in% west){
      final_table[c,]$state_abbreviation = 'W'
  } else if (final_table[c,]$state_abbreviation %in% midwest){
      final_table[c,]$state_abbreviation = 'M'
  } else if (final_table[c,]$state_abbreviation %in% northeast){
      final_table[c,]$state_abbreviation = 'N'
  } else {
      final_table = final_table[-c, ]
  }
}
final_table = final_table[final_table$state_abbreviation == 'S' | final_table$state_abbreviation == 'W' | final_table$state_abbreviation == 'M' | final_table$state_abbreviation == 'N',]

final_table$state_abbreviation <- as.factor(final_table$state_abbreviation)
head(final_table)
```

# Methods
```{r, warning=FALSE}
pruned_data = final_table[,-c(1:2)]

pruned_data = pruned_data[pruned_data$Hillary != 0,]
head(pruned_data)
```
* Loaded lmtest and corrplot libraries
* Removed unnecessary categorical variables such as location
* Cleaned data

## Initial Model

```{r}
initial_mod = lm(Hillary ~ RHI125214, data=pruned_data)
initial_mod
```
* Constructed an initial model that represents core of data

## Additive Models

```{r}
full_add_mod = lm(Hillary ~ ., data=pruned_data)
bptest(full_add_mod)
shapiro.test(resid(full_add_mod))
```
* Built a linear additive model with Hillary as the response and every variable as the predictor
* Checked initial assumptions with  Breusch-Pagan test for heteroscedasticity and found an extremely low p-value
* The high W value given from the Shapiro-Wilk test suggests a high level of normality

```{r}
corrplot(cor(pruned_data[,-1]), tl.cex = 0.5)
```

Looking at the coorelation plot above, some variables seem to be highly correlated with one another. Checking for multicollineartiy:

```{r}
b_mod = lm(Hillary ~ .-PST040210-POP010210-EDU685213, data=pruned_data)
b_mod_bic = step(full_add_mod, direction="backward", k=log(55), trace=0)
summary(b_mod)$r.squared
summary(full_add_mod)$r.squared
```



```{r}
n = length(coef(full_add_mod))

add_mod_back_aic = step(full_add_mod, direction="backward", k=2, trace=0)
add_mod_back_bic = step(full_add_mod, direction="backward", k=log(n), trace=0)

```
* The AIC and BIC models are created to remove unvaluable predictors 

```{r}
log_model = lm(log(Hillary) ~ ., data=pruned_data)

plot(fitted(log_model), resid(log_model), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Big Mod")

```

* The plot of the large additive model indicates that no major assumptions have been violates
* The model is a log model as we wanted to observe the exponentiated regression coefficients


```{r}
summary(full_add_mod)$r.squared
summary(add_mod_back_aic)$r.squared
summary(add_mod_back_bic)$r.squared

length(coef(full_add_mod))
length(coef(add_mod_back_aic))
length(coef(add_mod_back_bic))
```
* We compare the r{2} values from each of the models created so far
* They are all relatively similar, with the AIC and BIC models showing slightly lower values

```{r}
plot(fitted(add_mod_back_aic), resid(add_mod_back_aic), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Big Mod")
abline(h = 0, col = "darkorange", lwd = 2)

hist(resid(add_mod_back_aic),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, add_mod_back_aic",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 50)

qqnorm(resid(add_mod_back_aic), main = "Normal Q-Q Plot, add_mod_back_aic", col = "darkgrey")
qqline(resid(add_mod_back_aic), col = "dodgerblue", lwd = 2)
```
Check the AIC model assumptions
* Scatter Plot
 + Fitted values against Residuals and find density of the plot to be mostly normal
* Plot Histogram
 + Shows high normality of the AIC model
* QQ-Plot
 + Shows a low average distance between error and regression line. However the small valleys, especially between the theoretical quantities of 1-2 show that there is a violation of normality.


```{r}
plot(fitted(add_mod_back_bic), resid(add_mod_back_bic), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Big Mod")
abline(h = 0, col = "darkorange", lwd = 2)

hist(resid(add_mod_back_bic),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, add_mod_back_bic",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 50)
qqnorm(resid(add_mod_back_bic), main = "Normal Q-Q Plot, add_mod_back_bic", col = "darkgrey")
qqline(resid(add_mod_back_bic), col = "dodgerblue", lwd = 2)
```
Check the BIC model assumptions
* Scatter Plot
 + Fitted values against Residuals. Very similar to AIC model, few more outliers
* Plot Histogram
 + High normaility, slightly right skewed.
* QQ-Plot
 + Residuals, generally, are relatively close to regression line, with small valleys. Similar to the AIC model, these valleys suggest lack of normality at points.
```{r}
anova(add_mod_back_aic, full_add_mod)
anova(add_mod_back_bic, full_add_mod)
```
* Anova compares the AIC and BIC models against the full additive model
* We choose the smaller model because of the higher p-value and therefore reject the null hypothesis


## Interaction Models

```{r}
full_int_mod = lm(Hillary ~ .^2, data=pruned_data)
```
* Created an interactive model full_int_mod which sees Hillary as the response and all other independent variables as predictors
* The resulting r{2} value of the interactive model is 0.5118
```{r}
a = anova(full_int_mod)["Pr(>F)"]
str_formula = paste(row.names(a)[which(a < 0.01)], collapse = " + ")

small_int_mod = lm(as.formula(paste("Hillary", str_formula, sep = " ~ ")), data = pruned_data)

```

* Used p-values less than threshold (0.01) to select valuable predictors
* Hand selected valuable interactions
 +Original model is too big for AIC or BIC

```{r}
plot(fitted(small_int_mod), resid(small_int_mod), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Small Int Mod")
abline(h = 0, col = "darkorange", lwd = 2)

hist(resid(small_int_mod),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, small_int_mod",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 50)

qqnorm(resid(small_int_mod), main = "Normal Q-Q Plot, small_int_mod", col = "darkgrey")
qqline(resid(small_int_mod), col = "dodgerblue", lwd = 2)
```
Check the Interactive model assumptions
- Residuals Plot
 + At every fitted value, the mean of the residuals is roughly 0, so linearity is satisifed. However, the constant variance assumption is invalid. 
- Plot Histogram
 + The histogram of residuals is rougly follows a normal distribution, leading to the normality assumption to be valid.
- QQ-Plot
 + As the Q-Q plot nearly follows a straight line, this suggests the data does in fact come from a normal distribution.  
 

```{r}
anova(full_int_mod, small_int_mod)
```

Analyze the difference between the large interactive model and the small interactive model
* The F-Statistic is 1.5 with a p-value of 0.0038
* We reject the null and choose the smaller model
 + Shows us that less interactions with more significant independent predictors will yield a higher r{2} value and more normality

# Results


```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```


```{r}
summary(full_int_mod)$r.squared
summary(small_int_mod)$r.squared
summary(full_add_mod)$r.squared
summary(add_mod_back_aic)$r.squared
summary(add_mod_back_bic)$r.squared
summary(initial_mod)$r.squared

```


```{r}
calc_loocv_rmse(full_int_mod)
calc_loocv_rmse(small_int_mod)
calc_loocv_rmse(full_add_mod)
calc_loocv_rmse(add_mod_back_aic)
calc_loocv_rmse(add_mod_back_bic)
calc_loocv_rmse(initial_mod)
calc_loocv_rmse(b_mod)
calc_loocv_rmse(b_mod_bic)
```

### Finding VIF percentages
```{r}
(full_add_mod_vif = vif(full_add_mod))

length(which(full_add_mod_vif > 5)) / length(coef(full_add_mod))
```

```{r}
length(which(vif(add_mod_back_aic) > 5)) / length(coef(add_mod_back_aic))
length(which(vif(add_mod_back_bic) > 5)) / length(coef(add_mod_back_bic))
length(which(vif(small_int_mod) > 5)) / length(coef(small_int_mod))
```

In our additive models, we see a lot of collineraity between the predictors. This can be explained by the fact that the predictors themselves are naturally related to one another. For instance, people with Bachelor degrees also will have a high school dipoma. When looking at the interactive model, the high percentage of VIF values reflect the correlation between the interaction term, and both of the individual terms. 

# Discussion


# Appendix

