---
title: 'Team 420 Final Project'
author: "Ruben Verghese, Simer Singh, Gautam Ajjarapu, James Wei"
date: 'December 14, 2020'
output:
  html_document:
    theme: readable
    toc: yes
---
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

# Introduction
```{r}
library(lmtest)
library(car)
```

## Data Cleaning
```{r, warning = FALSE, message = FALSE}
data = read.csv("county_facts.csv", stringsAsFactors = FALSE)
results = read.csv("primary_results.csv", stringsAsFactors = FALSE)
library(stringr)
# Read in the data and results from their respective .csv files.
predictors = data[data$state_abbreviation != "", ]
# Remove rows that do not have a state abbreviation.
for (i in 1:nrow(predictors)) { 
  predictors$area_name[i] = str_replace(predictors$area_name[i], " County", "")
  predictors$area_name[i] = str_replace(predictors$area_name[i], " Parish", "")
}
# Change some of the naming schemes in the predictor .csv file because their are mismatches in the naming conventions
# between the two .csv files.
for (i in 1:nrow(predictors)) {
  state = predictors$state_abbreviation[i]
  fips = predictors$fips[i]
  frac_vote = results[results$state_abbreviation == state & results$fips == fips & results$candidate == "Hillary Clinton",]$fraction_votes
  #print(frac_vote)
  if (length(frac_vote) == 0 || is.na(frac_vote)) {
    #print(state)
    #print(fips)
  } else {
    predictors$Hillary[i] = frac_vote
  }
  county = predictors$area_name[i]
  frac_vote = results[results$county == county & results$candidate == "Hillary Clinton",]$fraction_votes
  if (predictors$Hillary[i] == 0) {
    if (length(frac_vote) == 0 || is.na(frac_vote)) {
    #print(county)
    } else {
    predictors$Hillary[i] = frac_vote
    }
  }
}
# This for loop attaches a new column to the end of the predictors .csv file witht the proportion of Hillary Votes.

final_table = predictors[predictors$Hillary != 0 | predictors$state_abbreviation != "DC", ]
# Remove DC because it adds a new binary variable due to one-hot encoding and only has one respective row. Kind of a # # waste.

final_table$Hillary <- final_table$Hillary * 100 # convert to percent

south = c('TX', 'OK', 'LA', 'AR', 'MS', 'AL', 'TN', 'KY', 'WV', 'VA', 'NC', 'SC', 'GA', 'AL', 'FL', 'MD', 'DC', 'DE')
west = c('CA', 'OR', 'WA', 'ID', 'MT', 'NV', 'AZ', 'UT', 'CO', 'NM', 'WY', 'AK')
midwest = c('ND', 'SD', 'NE', 'KS', 'MN', 'IA', 'MO', 'WI', 'IL', 'IN', 'MI', 'OH')
northeast = c('PA', 'NJ', 'CT', 'RI', 'MA', 'VT', 'NY', 'NH', 'ME')
# Here we break down the states into regions because this way we can reduce the dimensionality of the data. Without
# This modification, the full interaction model would have more coefficients than we have data points (rows) in our 
# dataframe. This is because of the one-hot encoding of our categorical variable (state). While this categorization
# Does result in a loss of information, it does make future computations easier and allows for p-values to exist, #etc.

for (c in 1:nrow(final_table)) {
  if (final_table[c,]$state_abbreviation %in% south) {
      final_table[c,]$state_abbreviation = 'S'
  } else if (final_table[c,]$state_abbreviation %in% west){
      final_table[c,]$state_abbreviation = 'W'
  } else if (final_table[c,]$state_abbreviation %in% midwest){
      final_table[c,]$state_abbreviation = 'M'
  } else if (final_table[c,]$state_abbreviation %in% northeast){
      final_table[c,]$state_abbreviation = 'N'
  } else {
      final_table = final_table[-c, ]
  }
}
final_table = final_table[final_table$state_abbreviation == 'S' | final_table$state_abbreviation == 'W' | final_table$state_abbreviation == 'M' | final_table$state_abbreviation == 'N',]

final_table$state_abbreviation <- as.factor(final_table$state_abbreviation) #Convert categorical to factor.
head(final_table)
```


166 incorrect rows out of 3143 so maybe just drop all of those.

**Final Table has been created**

# Methods
```{r, warning=FALSE}
library(corrplot)
pruned_data = final_table[,-c(1:2)]

pruned_data = pruned_data[pruned_data$Hillary != 0,]
pruned_data$state_abbreviation
pruned_data
```
* Loaded lmtest and corrplot libraries
* Removed unnecessary categorical variables such as location
* Cleaned data

# Paramaterization

# Initial Model

```{r}
initial_mod = lm(Hillary ~ RHI125214, data=pruned_data)
initial_mod
```
* Constructed an initial model that represents core of data

# Additive Models

```{r}
model = lm(Hillary ~ ., data=pruned_data)
bptest(model)
shapiro.test(resid(model))
```
* Built a linear additive model with Hillary as the response and every variable as the predictor
* Checked initial assumptions with  Breusch-Pagan test for heteroscedasticity and found an extremely low p-value
* The high W value given from the Shapiro-Wilk test suggests a high level of normality
```{r}
n = length(coef(model))

model_back_aic = step(model, direction="backward", k=2, trace=0)
model_back_bic = step(model, direction="backward", k=log(n), trace=0)

```
* The AIC and BIC models are created to remove unvaluable predictors 

```{r}
log_model = lm(log(Hillary) ~ ., data=pruned_data)

plot(fitted(log_model), resid(log_model), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Big Mod")

```
* The plot of the large additive model indicates that no major assumptions have been violates
* The model is a log model as we wanted to observe the exponentiated regression coefficients

```{r}
summary(model)$r.squared
summary(model_back_aic)$r.squared
summary(model_back_bic)$r.squared

length(coef(model))
length(coef(model_back_aic))
length(coef(model_back_bic))
```
* We compare the r{2} values from each of the models created so far
* They are all relatively similar, with the AIC and BIC models showing slightly lower values

```{r}
summary(model_back_aic)$r.squared
plot(fitted(model_back_aic), resid(model_back_aic), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Big Mod")
abline(h = 0, col = "darkorange", lwd = 2)

hist(resid(model_back_aic),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, model_back_aic",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 50)

qqnorm(resid(model_back_aic), main = "Normal Q-Q Plot, model_back_aic", col = "darkgrey")
qqline(resid(model_back_aic), col = "dodgerblue", lwd = 2)
```
Check the AIC model assumptions
1. r{2} value
 + 0.4344
2. Scatter Plot
 + Fitted values against Residuals and find density of the plot to be mostly normal
3. Plot Histogram
 + Shows high normality of the AIC model
4. QQ-Plot
 + Shows a low average distance between error and regression line. However the small valleys, especially between the theoretical quantities of 1-2 show that there is a violation of normality.


```{r}
summary(model_back_bic)$r.squared
plot(fitted(model_back_bic), resid(model_back_bic), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Big Mod")
abline(h = 0, col = "darkorange", lwd = 2)

hist(resid(model_back_bic),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, model_back_bic",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 50)
qqnorm(resid(model_back_bic), main = "Normal Q-Q Plot, model_back_bic", col = "darkgrey")
qqline(resid(model_back_bic), col = "dodgerblue", lwd = 2)
```
Check the BIC model assumptions
1. r{2} value
 + 0.4327
2. Scatter Plot
 + Fitted values against Residuals. Very similar to AIC model, few more outliers
3. Plot Histogram
 + High normaility, slightly right skewed.
4. QQ-Plot
 + Residuals, generally, are relatively close to regression line, with small valleys. Similar to the AIC model, these valleys suggest lack of normality at points.
```{r}
anova(model_back_aic, model)
anova(model_back_bic, model)
```
* Anova compares the AIC and BIC models against the full additive model
* We choose the smaller model because of the higher p-value and therefore reject the null hypothesis


## Interaction Models

```{r}
#big_mod_quad = lm(Hillary ~ .^2 + I(PST045214^2)+I(PST040210^2)+I(PST120214^2)+I(POP010210^2)+I(AGE135214^2)+I(AGE295214^2)+I(AGE775214^2)+I(SEX255214^2)+I(RHI125214^2)+I(RHI225214^2)+I(RHI325214^2)+I(RHI425214^2)+I(RHI525214^2)+I(RHI625214^2)+I(RHI725214^2)+I(RHI825214^2)+I(POP715213^2)+I(POP645213^2)+I(POP815213^2)+I(EDU635213^2)+I(EDU685213^2)+I(VET605213^2)+I(LFE305213^2)+I(HSG010214^2)+I(HSG445213^2)+I(HSG096213^2)+I(HSG495213^2)+I(HSD410213^2)+I(HSD310213^2)+I(INC910213^2)+I(INC110213^2)+I(PVY020213^2)+I(BZA010213^2)+I(BZA110213^2)+I(BZA115213^2)+I(NES010213^2)+I(SBO001207^2)+I(SBO315207^2)+I(SBO115207^2)+I(SBO215207^2)+I(SBO515207^2)+I(SBO415207^2)+I(SBO015207^2)+I(MAN450207^2)+I(WTN220207^2)+I(RTN130207^2)+I(RTN131207^2)+I(AFN120207^2)+I(BPS030214^2)+I(LND110210^2)+I(POP060210^2), data=pruned_data)

# entire interaction models

full_int_mod = lm(Hillary ~ .^2, data=pruned_data)
length(coef(full_int_mod))

#int_mod = lm(Hillary ~ (state_abbreviation + RHI125214 + RHI325214+ RHI225214+RHI425214+RHI525214+RHI625214+RHI725214+SBO001207+SBO315207+SBO115207+SBO215207+SBO515207+SBO415207+SBO015207+SEX255214+POP060210+PVY020213+HSG445213)^2, data=pruned_data)

#int_mod = lm(Hillary ~ state_abbreviation * (POP010210+AGE135214+AGE295214+AGE775214+SEX255214+RHI125214+RHI225214+RHI325214+RHI425214+RHI525214+RHI625214+RHI725214+RHI825214+POP715213+POP645213+POP815213+EDU635213+EDU685213+VET605213+LFE305213+HSG010214+HSG445213+HSG096213+HSG495213+HSD410213+HSD310213+INC910213+INC110213+PVY020213+BZA010213+BZA110213+BZA115213+NES010213+SBO001207+SBO315207), data=pruned_data)
#int_mod = lm(Hillary ~ (state_abbreviation + SBO115207)^2, data=pruned_data)

#summary(int_mod)$r.squared
```
* Created an interactive model full_int_mod which sees Hillary as the response and all other independent variables as predictors
* The resulting r{2} value of the interactive model is 0.5118
```{r}
a = anova(full_int_mod)["Pr(>F)"]
str_formula = paste(row.names(a)[which(a < 0.01)], collapse = " + ")

new_int_mod = lm(as.formula(paste("Hillary", str_formula, sep = " ~ ")), data = pruned_data)

```

* Used p-values less than threshold (0.01) to select valuable predictors
* Hand selected valuable interactions
 +Original model is too big for AIC or BIC

```{r}
summary(full_int_mod)$r.squared

plot(fitted(full_int_mod), resid(full_int_mod), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Big Mod")
abline(h = 0, col = "darkorange", lwd = 2)

hist(resid(full_int_mod),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, full_int_mod",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 50)

qqnorm(resid(full_int_mod), main = "Normal Q-Q Plot, full_int_mod", col = "darkgrey")
qqline(resid(full_int_mod), col = "dodgerblue", lwd = 2)
```
Check the Interactive model assumptions
1. r{2} value
 + 
2. Scatter Plot
 + 
3. Plot Histogram
 + 
4. QQ-Plot
 + 
```{r}
#outliers = rstandard(big_mod_quad)[abs(rstandard(big_mod_quad)) > 2]
#cooks.distance(big_mod_quad)[names(outliers)] > 4 / length(cooks.distance(big_mod_quad))
```



```{r}
small_model = lm(Hillary ~ (AGE295214 + AGE775214 + RHI125214 + RHI225214 + RHI625214 + RHI725214 + RHI825214 + POP815213 + EDU635213 + EDU685213 + LFE305213 + HSG096213 + HSG495213 + HSD410213 + HSD310213 + INC910213 + PVY020213 + BZA010213 + BZA110213 + NES010213 + SBO001207 + SBO315207 + SBO415207)^2 + I(AGE295214 ^2)+I( AGE775214 ^2)+I( RHI125214 ^2)+I( RHI225214 ^2)+I( RHI625214 ^2)+I( RHI725214 ^2)+I( RHI825214 ^2)+I( POP815213 ^2)+I( EDU635213 ^2)+I( EDU685213 ^2)+I( LFE305213 ^2)+I( HSG096213 ^2)+I( HSG495213 ^2)+I( HSD410213 ^2)+I( HSD310213 ^2)+I( INC910213 ^2)+I( PVY020213 ^2)+I( BZA010213 ^2)+I( BZA110213 ^2)+I( NES010213 ^2)+I( SBO001207 ^2)+I( SBO315207 ^2)+I( SBO415207^2), data = pruned_data)
summary(small_model)$r.squared

```

```{r}
anova(full_int_mod, new_int_mod)
```

Analyze the difference between the large interactive model and the small interactive model
* The F-Statistic is 1.5 with a p-value of 0.0038
* We reject the null and choose the smaller model
 + Shows us that less interactions with more significant independent predictors will yield a higher r{2} value and more normality

# Results


```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```


```{r}
summary(full_int_mod)$r.squared
summary(new_int_mod)$r.squared
summary(model)$r.squared
summary(model_back_aic)$r.squared
summary(model_back_bic)$r.squared
summary(initial_mod)$r.squared

```

```{r}
vif(new_int_mod)
vif(model)
vif(model_back_aic)
vif(model_back_bic)
```


```{r}

calc_loocv_rmse(full_int_mod)
#calc_loocv_rmse(big_mod_quad)
calc_loocv_rmse(new_int_mod)
#calc_loocv_rmse(int_model_back_aic)
#calc_loocv_rmse(int_model_back_bic)
calc_loocv_rmse(model)
calc_loocv_rmse(model_back_aic)
calc_loocv_rmse(model_back_bic)
calc_loocv_rmse(initial_mod)


```


# Discussion


# Appendix

```{r}
#This code was meant to try and remove some of the states if they had less than a certain number of data points. We attempted to use it in order to correct the shapiro and bp tests but it did not make any difference.

# final_table = final_table[final_table$state_abbreviation == 'S' | final_table$state_abbreviation == 'W' | final_table$state_abbreviation == 'M' | final_table$state_abbreviation == 'N',]
states = unique(final_table$state_abbreviation)
tot = 0
for (x in states) {
  if (nrow(final_table[final_table$state_abbreviation == x, ]) < 25) {
    final_table = final_table[final_table$state_abbreviation != x, ]
  }
  tot = tot + nrow(final_table[final_table$state_abbreviation == x, ])
}
tot = tot / 50
final_table$state_abbreviation <- as.factor(final_table$state_abbreviation)
```
