---
title: 'Team 420 Final Project'
author: "Ruben Verghese, Simer Singh, Gautam Ajjarapu, James Wei"
date: 'December 14, 2020'
output:
  html_document:
    theme: readable
    toc: yes
---
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

# Introduction
```{r, warning=FALSE}
library(lmtest)
library(car)
library(corrplot)
```

## Data Cleaning
```{r}
data = read.csv("county_facts.csv", stringsAsFactors = FALSE)
results = read.csv("primary_results.csv", stringsAsFactors = FALSE)
library(stringr)
predictors = data[data$state_abbreviation != "", ]
for (i in 1:nrow(predictors)) { 
  predictors$area_name[i] = str_replace(predictors$area_name[i], " County", "")
  predictors$area_name[i] = str_replace(predictors$area_name[i], " Parish", "")
  #print(predictors$area_name[i])
}

for (i in 1:nrow(predictors)) {
  state = predictors$state_abbreviation[i]
  fips = predictors$fips[i]
  frac_vote = results[results$state_abbreviation == state & results$fips == fips & results$candidate == "Hillary Clinton",]$fraction_votes
  #print(frac_vote)
  if (length(frac_vote) == 0 || is.na(frac_vote)) {
    #print(state)
    #print(fips)
  } else {
    predictors$Hillary[i] = frac_vote
  }
  county = predictors$area_name[i]
  frac_vote = results[results$county == county & results$candidate == "Hillary Clinton",]$fraction_votes
  if (predictors$Hillary[i] == 0) {
    if (length(frac_vote) == 0 || is.na(frac_vote)) {
    #print(county)
    } else {
    predictors$Hillary[i] = frac_vote
    }
  }
}

final_table = predictors[predictors$Hillary != 0 | predictors$state_abbreviation != "DC", ]

final_table$Hillary <- final_table$Hillary * 100 # convert to percent

south = c('TX', 'OK', 'LA', 'AR', 'MS', 'AL', 'TN', 'KY', 'WV', 'VA', 'NC', 'SC', 'GA', 'AL', 'FL', 'MD', 'DC', 'DE')
west = c('CA', 'OR', 'WA', 'ID', 'MT', 'NV', 'AZ', 'UT', 'CO', 'NM', 'WY', 'AK')
midwest = c('ND', 'SD', 'NE', 'KS', 'MN', 'IA', 'MO', 'WI', 'IL', 'IN', 'MI', 'OH')
northeast = c('PA', 'NJ', 'CT', 'RI', 'MA', 'VT', 'NY', 'NH', 'ME')

for (c in 1:nrow(final_table)) {
  if (final_table[c,]$state_abbreviation %in% south) {
      final_table[c,]$state_abbreviation = 'S'
  } else if (final_table[c,]$state_abbreviation %in% west){
      final_table[c,]$state_abbreviation = 'W'
  } else if (final_table[c,]$state_abbreviation %in% midwest){
      final_table[c,]$state_abbreviation = 'M'
  } else if (final_table[c,]$state_abbreviation %in% northeast){
      final_table[c,]$state_abbreviation = 'N'
  } else {
      final_table = final_table[-c, ]
  }
}
final_table = final_table[final_table$state_abbreviation == 'S' | final_table$state_abbreviation == 'W' | final_table$state_abbreviation == 'M' | final_table$state_abbreviation == 'N',]

final_table$state_abbreviation <- as.factor(final_table$state_abbreviation)
final_table
```

```{r}
head(data)
```



```{r}
head(data, 1)[3] == ""
```

```{r}
data$state_abbreviation[4]
```


```{r}
results[results$candidate == "Hillary Clinton", ]
```

```{r}
data[data$state_abbreviation != "", ]

predictors = data[data$state_abbreviation != "", ]
```

```{r}
for (i in 1:nrow(predictors)) { 
  predictors$area_name[i] = str_replace(predictors$area_name[i], " County", "")
  predictors$area_name[i] = str_replace(predictors$area_name[i], " Parish", "")
  #print(predictors$area_name[i])
}

#predictors$area_name[1]
```


```{r}
predictors$area_name[1]
```

```{r}
for (i in 1:nrow(predictors)) {
  state = predictors$state_abbreviation[i]
  fips = predictors$fips[i]
  frac_vote = results[results$state_abbreviation == state & results$fips == fips & results$candidate == "Hillary Clinton",]$fraction_votes
  #print(frac_vote)
  if (length(frac_vote) == 0 || is.na(frac_vote)) {
    #print(state)
    #print(fips)
  } else {
    predictors$Hillary[i] = frac_vote
  }
  county = predictors$area_name[i]
  frac_vote = results[results$county == county & results$candidate == "Hillary Clinton",]$fraction_votes
  if (predictors$Hillary[i] == 0) {
    if (length(frac_vote) == 0 || is.na(frac_vote)) {
    #print(county)
    } else {
    predictors$Hillary[i] = frac_vote
    }
  }
}

head(predictors, 2)
```

```{r}
temp = results[results$state_abbreviation == "AL" & results$fips == 1001 & results$candidate == "Hillary Clinton",]$fraction_votes

predictors[predictors$state_abbreviation == "VT",]
```
```{r}
predictors[predictors$Hillary == 0 & predictors$state_abbreviation != 'AK' & predictors$state_abbreviation != 'KS' & predictors$state_abbreviation != 'MN' & predictors$state_abbreviation != 'ME'& predictors$state_abbreviation != 'ND', ]

```

166 incorrect rows out of 3143 so maybe just drop all of those.

**Final Dataframe**

```{r}
#nrow(predictors)
final_table = predictors[predictors$Hillary != 0 | predictors$state_abbreviation != "DC", ]
#final_table$state_abbreviation <- as.factor(final_table$state_abbreviation)
final_table$Hillary <- final_table$Hillary * 100 # convert to percent
final_table
```
```{r}
south = c('TX', 'OK', 'LA', 'AR', 'MS', 'AL', 'TN', 'KY', 'WV', 'VA', 'NC', 'SC', 'GA', 'AL', 'FL', 'MD', 'DC', 'DE')
west = c('CA', 'OR', 'WA', 'ID', 'MT', 'NV', 'AZ', 'UT', 'CO', 'NM', 'WY', 'AK')
midwest = c('ND', 'SD', 'NE', 'KS', 'MN', 'IA', 'MO', 'WI', 'IL', 'IN', 'MI', 'OH')
northeast = c('PA', 'NJ', 'CT', 'RI', 'MA', 'VT', 'NY', 'NH', 'ME')
```


```{r, eval = FALSE}
for (c in 1:nrow(final_table)) {
  if (final_table[c,]$state_abbreviation %in% south) {
      final_table[c,]$state_abbreviation = 'S'
  } else if (final_table[c,]$state_abbreviation %in% west){
      final_table[c,]$state_abbreviation = 'W'
  } else if (final_table[c,]$state_abbreviation %in% midwest){
      final_table[c,]$state_abbreviation = 'M'
  } else if (final_table[c,]$state_abbreviation %in% northeast){
      final_table[c,]$state_abbreviation = 'N'
  } else {
      final_table = final_table[-c, ]
  }
}
```

```{r}


# final_table = final_table[final_table$state_abbreviation == 'S' | final_table$state_abbreviation == 'W' | final_table$state_abbreviation == 'M' | final_table$state_abbreviation == 'N',]
states = unique(final_table$state_abbreviation)
tot = 0
for (x in states) {
  if (nrow(final_table[final_table$state_abbreviation == x, ]) < 25) {
    final_table = final_table[final_table$state_abbreviation != x, ]
  }
  tot = tot + nrow(final_table[final_table$state_abbreviation == x, ])
}
tot = tot / 50
final_table$state_abbreviation <- as.factor(final_table$state_abbreviation)
```


# Methods
```{r, warning=FALSE}
pruned_data = final_table[,-c(1:2)]

pruned_data = pruned_data[pruned_data$Hillary != 0,]
pruned_data$state_abbreviation
pruned_data
```
* Loaded lmtest and corrplot libraries
* Removed unnecessary categorical variables such as location
* Cleaned data



# Initial Model

```{r}
initial_mod = lm(Hillary ~ RHI125214, data=pruned_data)
initial_mod
```
* Constructed an initial model that represents core of data

# Additive Models

```{r}
full_add_mod = lm(Hillary ~ ., data=pruned_data)
bptest(full_add_mod)
shapiro.test(resid(full_add_mod))
```
* Built a linear additive model with Hillary as the response and every variable as the predictor
* Checked initial assumptions with  Breusch-Pagan test for heteroscedasticity and found an extremely low p-value
* The high W value given from the Shapiro-Wilk test suggests a high level of normality

```{r}
n = length(coef(model))

add_mod_back_aic = step(full_add_mod, direction="backward", k=2, trace=0)
add_mod_back_bic = step(full_add_mod, direction="backward", k=log(n), trace=0)

```
* The AIC and BIC models are created to remove unvaluable predictors 

```{r}
log_model = lm(log(Hillary) ~ ., data=pruned_data)

plot(fitted(log_model), resid(log_model), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Big Mod")

```

* The plot of the large additive model indicates that no major assumptions have been violates
* The model is a log model as we wanted to observe the exponentiated regression coefficients


```{r}
summary(full_add_mod)$r.squared
summary(add_mod_back_aic)$r.squared
summary(add_mod_back_bic)$r.squared

length(coef(full_add_mod))
length(coef(add_mod_back_aic))
length(coef(add_mod_back_bic))
```
* We compare the r{2} values from each of the models created so far
* They are all relatively similar, with the AIC and BIC models showing slightly lower values

```{r}
plot(fitted(add_mod_back_aic), resid(add_mod_back_aic), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Big Mod")
abline(h = 0, col = "darkorange", lwd = 2)

hist(resid(add_mod_back_aic),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, add_mod_back_aic",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 50)

qqnorm(resid(add_mod_back_aic), main = "Normal Q-Q Plot, add_mod_back_aic", col = "darkgrey")
qqline(resid(add_mod_back_aic), col = "dodgerblue", lwd = 2)
```
Check the AIC model assumptions
1. r{2} value
 + 0.4344
2. Scatter Plot
 + Fitted values against Residuals and find density of the plot to be mostly normal
3. Plot Histogram
 + Shows high normality of the AIC model
4. QQ-Plot
 + Shows a low average distance between error and regression line. However the small valleys, especially between the theoretical quantities of 1-2 show that there is a violation of normality.


```{r}
plot(fitted(add_mod_back_bic), resid(add_mod_back_bic), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Big Mod")
abline(h = 0, col = "darkorange", lwd = 2)

hist(resid(add_mod_back_bic),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, add_mod_back_bic",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 50)
qqnorm(resid(add_mod_back_bic), main = "Normal Q-Q Plot, add_mod_back_bic", col = "darkgrey")
qqline(resid(add_mod_back_bic), col = "dodgerblue", lwd = 2)
```
Check the BIC model assumptions
1. r{2} value
 + 0.4327
2. Scatter Plot
 + Fitted values against Residuals. Very similar to AIC model, few more outliers
3. Plot Histogram
 + High normaility, slightly right skewed.
4. QQ-Plot
 + Residuals, generally, are relatively close to regression line, with small valleys. Similar to the AIC model, these valleys suggest lack of normality at points.
```{r}
anova(add_mod_back_aic, model)
anova(add_mod_back_bic, model)
```
* Anova compares the AIC and BIC models against the full additive model
* We choose the smaller model because of the higher p-value and therefore reject the null hypothesis


## Interaction Models

```{r}
full_int_mod = lm(Hillary ~ .^2, data=pruned_data)
```
* Created an interactive model full_int_mod which sees Hillary as the response and all other independent variables as predictors
* The resulting r{2} value of the interactive model is 0.5118
```{r}
a = anova(full_int_mod)["Pr(>F)"]
str_formula = paste(row.names(a)[which(a < 0.01)], collapse = " + ")

small_int_mod = lm(as.formula(paste("Hillary", str_formula, sep = " ~ ")), data = pruned_data)

```

* Used p-values less than threshold (0.01) to select valuable predictors
* Hand selected valuable interactions
 +Original model is too big for AIC or BIC

```{r}
plot(fitted(new_int_mod), resid(new_int_mod), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Small Int Mod")
abline(h = 0, col = "darkorange", lwd = 2)

hist(resid(new_int_mod),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, small_int_mod",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 50)

qqnorm(resid(new_int_mod), main = "Normal Q-Q Plot, small_int_mod", col = "darkgrey")
qqline(resid(new_int_mod), col = "dodgerblue", lwd = 2)
```
Check the Interactive model assumptions
1. r{2} value
 + `r small_int_mod_r2`
2. Residuals Plot
 + At every fitted value, the mean of the residuals is roughly 0, so linearity is satisifed. However, the constant variance assumption is invalid. 
3. Plot Histogram
 + The histogram of residuals is rougly follows a normal distribution, leading to the normality assumption to be valid.
4. QQ-Plot
 + As the Q-Q plot nearly follows a straight line, this suggests the data does in fact come from a normal distribution.  
 
```{r}
#outliers = rstandard(big_mod_quad)[abs(rstandard(big_mod_quad)) > 2]
#cooks.distance(big_mod_quad)[names(outliers)] > 4 / length(cooks.distance(big_mod_quad))
```


```{r}
anova(full_int_mod, new_int_mod)
```

Analyze the difference between the large interactive model and the small interactive model
* The F-Statistic is 1.5 with a p-value of 0.0038
* We reject the null and choose the smaller model
 + Shows us that less interactions with more significant independent predictors will yield a higher r{2} value and more normality

# Results


```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```


```{r}
summary(full_int_mod)$r.squared
summary(new_int_mod)$r.squared
summary(model)$r.squared
summary(add_mod_back_aic)$r.squared
summary(add_mod_back_bic)$r.squared
summary(initial_mod)$r.squared

```

```{r}
vif(new_int_mod)
vif(full_add_mod)
vif(add_mod_back_aic)
vif(add_mod_back_bic)
```


```{r}

calc_loocv_rmse(full_int_mod)
calc_loocv_rmse(new_int_mod)
calc_loocv_rmse(full_add_mod)
calc_loocv_rmse(add_mod_back_aic)
calc_loocv_rmse(add_mod_back_bic)
calc_loocv_rmse(initial_mod)
```


# Discussion


# Appendix

